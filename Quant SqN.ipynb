{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quantised SqueezeNet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import builtins\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.jit as jit\n",
    "import torch.backends.cudnn as cudnn\n",
    "from torch.autograd import Variable\n",
    "\n",
    "# Workaround for \"RuntimeError: Set changed size during iteration\"?\n",
    "import tqdm\n",
    "tqdm.monitor_interval = 0\n",
    "\n",
    "import itertools\n",
    "from copy import deepcopy\n",
    "from inspect import getfullargspec\n",
    "\n",
    "# Remember to:\n",
    "!export PYTHONPATH=$(readlink -m ./pytorch-playground):$PYTHONPATH\n",
    "from utee import misc, quant, selector\n",
    "from imagenet import squeezenet\n",
    "from collections import OrderedDict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup GPU and load the original model\n",
    "Make sure the ImageNet data is downloaded and converted as detailed [here](./README.md/#imagenet-dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting GPU: ['0']\n",
      "Building and initializing squeezenet_v1 parameters\n"
     ]
    }
   ],
   "source": [
    "gpu = misc.auto_select_gpu(utility_bound=0, num_gpu=1, selected_gpus='0')\n",
    "ngpu = len(gpu)\n",
    "input_size = 224\n",
    "\n",
    "batch_size = 100\n",
    "data_root='/tmp/public_dataset/pytorch/'\n",
    "\n",
    "assert torch.cuda.is_available(), 'no cuda'\n",
    "torch.manual_seed(117)\n",
    "torch.cuda.manual_seed(117)\n",
    "\n",
    "# load model and dataset fetcher\n",
    "model_orig, ds_fetcher, is_imagenet = selector.select('squeezenet_v1', model_root='~/.torch/models')\n",
    "model_orig = model_orig.train(False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate the original model with float32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading pickle object from /tmp/public_dataset/pytorch/imagenet-data/val224.pkl\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building IMAGENET data loader: 50,000 for train; 50,000 for test\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "=> Done (5.9212 s)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "645232bd2336435cacea948478648534",
       "version_major": 2,
       "version_minor": 0
      },
      "text/html": [
       "<p>Failed to display Jupyter Widget of type <code>HBox</code>.</p>\n",
       "<p>\n",
       "  If you're reading this message in the Jupyter Notebook or JupyterLab Notebook, it may mean\n",
       "  that the widgets JavaScript is still loading. If this message persists, it\n",
       "  likely means that the widgets JavaScript library is either not installed or\n",
       "  not enabled. See the <a href=\"https://ipywidgets.readthedocs.io/en/stable/user_install.html\">Jupyter\n",
       "  Widgets Documentation</a> for setup instructions.\n",
       "</p>\n",
       "<p>\n",
       "  If you're reading this message in another frontend (for example, a static\n",
       "  rendering on GitHub or <a href=\"https://nbviewer.jupyter.org/\">NBViewer</a>),\n",
       "  it may mean that your frontend doesn't currently support widgets.\n",
       "</p>\n"
      ],
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=500), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original model accuracy: (0.55948, 0.7913)\n"
     ]
    }
   ],
   "source": [
    "# Eval model (this is the accuracy at fp32)\n",
    "val_ds = ds_fetcher(batch_size, data_root=data_root, train=False, input_size=input_size)\n",
    "acc1, acc5 = misc.eval_model(model_orig, val_ds, ngpu=ngpu, is_imagenet=is_imagenet)\n",
    "print(f'Original model accuracy: ({acc1}, {acc5})')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate the original model with float16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading pickle object from /tmp/public_dataset/pytorch/imagenet-data/val224.pkl\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building IMAGENET data loader: 50,000 for train; 50,000 for test\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "=> Done (5.8936 s)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5d97ea39eaee4adda28903d19d378cba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/html": [
       "<p>Failed to display Jupyter Widget of type <code>HBox</code>.</p>\n",
       "<p>\n",
       "  If you're reading this message in the Jupyter Notebook or JupyterLab Notebook, it may mean\n",
       "  that the widgets JavaScript is still loading. If this message persists, it\n",
       "  likely means that the widgets JavaScript library is either not installed or\n",
       "  not enabled. See the <a href=\"https://ipywidgets.readthedocs.io/en/stable/user_install.html\">Jupyter\n",
       "  Widgets Documentation</a> for setup instructions.\n",
       "</p>\n",
       "<p>\n",
       "  If you're reading this message in another frontend (for example, a static\n",
       "  rendering on GitHub or <a href=\"https://nbviewer.jupyter.org/\">NBViewer</a>),\n",
       "  it may mean that your frontend doesn't currently support widgets.\n",
       "</p>\n"
      ],
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=500), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in thread Thread-4:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ubuntu/anaconda3/envs/pytorch_p36/lib/python3.6/threading.py\", line 916, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/home/ubuntu/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/tqdm/_tqdm.py\", line 144, in run\n",
      "    for instance in self.tqdm_cls._instances:\n",
      "  File \"/home/ubuntu/anaconda3/envs/pytorch_p36/lib/python3.6/_weakrefset.py\", line 60, in __iter__\n",
      "    for itemref in self.data:\n",
      "RuntimeError: Set changed size during iteration\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test accuracy of (0.55962,0.79128), a drop of (-0.000140000000000029,2.0000000000020002e-05) for float16\n"
     ]
    }
   ],
   "source": [
    "model_half = deepcopy(model_orig).half()\n",
    "val_ds = ds_fetcher(batch_size, data_root=data_root, train=False, input_size=input_size)\n",
    "acc1h, acc5h = misc.eval_model(model_half, val_ds, ngpu=ngpu, is_imagenet=is_imagenet, data_fn=(lambda x: x.half()))\n",
    "print(f'Test accuracy of ({acc1h},{acc5h}), a drop of ({acc1-acc1h},{acc5-acc5h}) for float16')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Infrastructure for changing models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_apply(model, fn, clone=True):\n",
    "    \"\"\"\n",
    "    Recursively map a Module.\n",
    "    \"\"\"\n",
    "    def _apply(m, path):\n",
    "        if len(getfullargspec(fn).args) > 1:\n",
    "            um = fn(m, path or '/')\n",
    "        else:\n",
    "            um = fn(m)\n",
    "            \n",
    "        if um is not None:\n",
    "            return um\n",
    "        else:\n",
    "            for (k, v) in list(m._modules.items()):\n",
    "                m._modules[k] = _apply(v, f'{path}/{k}')\n",
    "            return m\n",
    "    return _apply(deepcopy(model) if clone else model, '')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split convolutions into conv+add, and TODO fold batchnorm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conv2d split into Conv2d + AddBias\n"
     ]
    }
   ],
   "source": [
    "class AddBias(nn.Module):\n",
    "    def __init__(self, num_features):\n",
    "        super(AddBias, self).__init__()\n",
    "        self.num_features = num_features\n",
    "        self.bias = nn.Parameter(torch.Tensor(num_features))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return x + self.bias.view([1, self.num_features, 1, 1])\n",
    "    \n",
    "def split_conv(m):\n",
    "    if isinstance(m, nn.Conv2d):\n",
    "        conv = nn.Conv2d(m.in_channels, m.out_channels, m.kernel_size,\n",
    "                         stride=m.stride, padding=m.padding,\n",
    "                         dilation=m.dilation, groups=m.groups, bias=False)\n",
    "        conv.weight = m.weight\n",
    "        addbias = AddBias(m.out_channels)\n",
    "        addbias.bias = m.bias\n",
    "        return nn.Sequential(OrderedDict([('conv', conv), ('addbias', addbias)]))\n",
    "model_raw = model_apply(model_orig, split_conv)\n",
    "print(f'Conv2d split into Conv2d + AddBias')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Collect activation statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function for inserting loggers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO is this better done with register_forward_hook?\n",
    "class Logger(nn.Module):\n",
    "    def __init__(self, name):\n",
    "        super(Logger, self).__init__()\n",
    "        #print(f'Creating logger for \"{name}\"')\n",
    "        self.name = name\n",
    "        self.log_items = np.zeros((0, 2))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # (~4.9it/s)\n",
    "        # NOTE this takes the min/max of the entire batch (100 images) \n",
    "        log_item = [ torch.min(x).data.cpu().numpy()[0],\n",
    "                     torch.max(x).data.cpu().numpy()[0] ]\n",
    "        self.log_items = np.append(self.log_items, log_item)                \n",
    "        return x\n",
    "    \n",
    "def duplicate_model_with_logging(model):\n",
    "    loggers = {}\n",
    "    def insert_logger(m, path):\n",
    "        if isinstance(m, (nn.Conv2d, AddBias, nn.Linear, nn.BatchNorm1d, nn.BatchNorm2d, nn.AvgPool2d)):\n",
    "            log_in  = Logger(f'{path}/log_in')\n",
    "            log_out = Logger(f'{path}/log_out')\n",
    "            loggers[log_in.name]  = log_in\n",
    "            loggers[log_out.name] = log_out\n",
    "            return nn.Sequential(log_in, m, log_out)\n",
    "    loggers['input'] = Logger('input')\n",
    "    r = nn.Sequential(loggers['input'],\n",
    "                      model_apply(model, insert_logger))\n",
    "    return r, loggers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create copy of model with logging, and collect stats over the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading pickle object from /tmp/public_dataset/pytorch/imagenet-data/val224.pkl\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building IMAGENET data loader: 50,000 for train; 50,000 for test\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "=> Done (5.9518 s)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Only using 500 for stats-collection\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a2cf738c10ce4b65ac3d368337a13b5e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/html": [
       "<p>Failed to display Jupyter Widget of type <code>HBox</code>.</p>\n",
       "<p>\n",
       "  If you're reading this message in the Jupyter Notebook or JupyterLab Notebook, it may mean\n",
       "  that the widgets JavaScript is still loading. If this message persists, it\n",
       "  likely means that the widgets JavaScript library is either not installed or\n",
       "  not enabled. See the <a href=\"https://ipywidgets.readthedocs.io/en/stable/user_install.html\">Jupyter\n",
       "  Widgets Documentation</a> for setup instructions.\n",
       "</p>\n",
       "<p>\n",
       "  If you're reading this message in another frontend (for example, a static\n",
       "  rendering on GitHub or <a href=\"https://nbviewer.jupyter.org/\">NBViewer</a>),\n",
       "  it may mean that your frontend doesn't currently support widgets.\n",
       "</p>\n"
      ],
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=5), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy of (0.562,0.794), a drop of (-0.0025200000000000777,-0.0027000000000000357)\n",
      "ERROR: transforms have changed accuracy\n"
     ]
    }
   ],
   "source": [
    "# Insert loggers\n",
    "model_log, loggers = duplicate_model_with_logging(model_raw)\n",
    "# Run over the test set to collect stats\n",
    "val_ds = ds_fetcher(batch_size, data_root=data_root, train=False, input_size=input_size)\n",
    "# Only collect stats from part of the dataset\n",
    "n_batch = int(val_ds.n_batch * 0.01)\n",
    "print(f'Only using {val_ds.n_batch} for stats-collection')\n",
    "\n",
    "# Collect stats\n",
    "acc1s, acc5s = misc.eval_model(model_log, val_ds, ngpu=ngpu, is_imagenet=is_imagenet, n_sample=n_batch)\n",
    "print(f'Test accuracy of ({acc1s},{acc5s}), a drop of ({acc1-acc1s},{acc5-acc5s})')\n",
    "if (acc1 - acc1s) != 0 or (acc5 - acc5s) != 0:\n",
    "    print(\"ERROR: transforms have changed accuracy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get scale factors from the collected stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'/classifier/1/addbias/log_in': 2.1596730149637056,\n",
       " '/classifier/1/addbias/log_out': 2.1598191148652806,\n",
       " '/classifier/1/conv/log_in': 4.711094653512549,\n",
       " '/classifier/1/conv/log_out': 2.1596730149637056,\n",
       " '/classifier/3/log_in': 2.1598191148652806,\n",
       " '/classifier/3/log_out': 0.34802837822380966,\n",
       " '/features/0/addbias/log_in': 0.14483134953055796,\n",
       " '/features/0/addbias/log_out': 0.14690978883758304,\n",
       " '/features/0/conv/log_in': 0.020787402400820273,\n",
       " '/features/0/conv/log_out': 0.14483134953055796,\n",
       " '/features/10/group1/squeeze/addbias/log_in': 6.9815669022207185,\n",
       " '/features/10/group1/squeeze/addbias/log_out': 6.981395811546506,\n",
       " '/features/10/group1/squeeze/conv/log_in': 3.7115569828063486,\n",
       " '/features/10/group1/squeeze/conv/log_out': 6.9815669022207185,\n",
       " '/features/10/group2/expand1x1/addbias/log_in': 3.638255201925443,\n",
       " '/features/10/group2/expand1x1/addbias/log_out': 3.6382376603254185,\n",
       " '/features/10/group2/expand1x1/conv/log_in': 6.981395811546506,\n",
       " '/features/10/group2/expand1x1/conv/log_out': 3.638255201925443,\n",
       " '/features/10/group3/expand3x3/addbias/log_in': 6.351740318959154,\n",
       " '/features/10/group3/expand3x3/addbias/log_out': 6.351740799550935,\n",
       " '/features/10/group3/expand3x3/conv/log_in': 6.981395811546506,\n",
       " '/features/10/group3/expand3x3/conv/log_out': 6.351740318959154,\n",
       " '/features/11/group1/squeeze/addbias/log_in': 10.656203863188976,\n",
       " '/features/11/group1/squeeze/addbias/log_out': 10.655975101500983,\n",
       " '/features/11/group1/squeeze/conv/log_in': 5.180389164000984,\n",
       " '/features/11/group1/squeeze/conv/log_out': 10.656203863188976,\n",
       " '/features/11/group2/expand1x1/addbias/log_in': 4.7392823226808565,\n",
       " '/features/11/group2/expand1x1/addbias/log_out': 4.7391491987573815,\n",
       " '/features/11/group2/expand1x1/conv/log_in': 10.655975101500983,\n",
       " '/features/11/group2/expand1x1/conv/log_out': 4.7392823226808565,\n",
       " '/features/11/group3/expand3x3/addbias/log_in': 7.224388302780512,\n",
       " '/features/11/group3/expand3x3/addbias/log_out': 7.224267193651575,\n",
       " '/features/11/group3/expand3x3/conv/log_in': 10.655975101500983,\n",
       " '/features/11/group3/expand3x3/conv/log_out': 7.224388302780512,\n",
       " '/features/12/group1/squeeze/addbias/log_in': 8.16595122570128,\n",
       " '/features/12/group1/squeeze/addbias/log_out': 8.164420060285433,\n",
       " '/features/12/group1/squeeze/conv/log_in': 5.360257847102608,\n",
       " '/features/12/group1/squeeze/conv/log_out': 8.16595122570128,\n",
       " '/features/12/group2/expand1x1/addbias/log_in': 6.22149658203125,\n",
       " '/features/12/group2/expand1x1/addbias/log_out': 6.2214600570558565,\n",
       " '/features/12/group2/expand1x1/conv/log_in': 8.164420060285433,\n",
       " '/features/12/group2/expand1x1/conv/log_out': 6.22149658203125,\n",
       " '/features/12/group3/expand3x3/addbias/log_in': 9.469913993294783,\n",
       " '/features/12/group3/expand3x3/addbias/log_out': 9.469892847256398,\n",
       " '/features/12/group3/expand3x3/conv/log_in': 8.164420060285433,\n",
       " '/features/12/group3/expand3x3/conv/log_out': 9.469913993294783,\n",
       " '/features/3/group1/squeeze/addbias/log_in': 0.3909072575606699,\n",
       " '/features/3/group1/squeeze/addbias/log_out': 0.3836523641751507,\n",
       " '/features/3/group1/squeeze/conv/log_in': 0.14690978883758304,\n",
       " '/features/3/group1/squeeze/conv/log_out': 0.3909072575606699,\n",
       " '/features/3/group2/expand1x1/addbias/log_in': 0.29981162604384537,\n",
       " '/features/3/group2/expand1x1/addbias/log_out': 0.2993703526774729,\n",
       " '/features/3/group2/expand1x1/conv/log_in': 0.3178577573280635,\n",
       " '/features/3/group2/expand1x1/conv/log_out': 0.29981162604384537,\n",
       " '/features/3/group3/expand3x3/addbias/log_in': 0.41897688137264705,\n",
       " '/features/3/group3/expand3x3/addbias/log_out': 0.4190451554426058,\n",
       " '/features/3/group3/expand3x3/conv/log_in': 0.3178577573280635,\n",
       " '/features/3/group3/expand3x3/conv/log_out': 0.41897688137264705,\n",
       " '/features/4/group1/squeeze/addbias/log_in': 0.44665073785256215,\n",
       " '/features/4/group1/squeeze/addbias/log_out': 0.4447247362512303,\n",
       " '/features/4/group1/squeeze/conv/log_in': 0.26868540846456695,\n",
       " '/features/4/group1/squeeze/conv/log_out': 0.44665073785256215,\n",
       " '/features/4/group2/expand1x1/addbias/log_in': 0.43873034499761626,\n",
       " '/features/4/group2/expand1x1/addbias/log_out': 0.43653551236851007,\n",
       " '/features/4/group2/expand1x1/conv/log_in': 0.4447247362512303,\n",
       " '/features/4/group2/expand1x1/conv/log_out': 0.43873034499761626,\n",
       " '/features/4/group3/expand3x3/addbias/log_in': 0.6298540370670829,\n",
       " '/features/4/group3/expand3x3/addbias/log_out': 0.6296296007051243,\n",
       " '/features/4/group3/expand3x3/conv/log_in': 0.4447247362512303,\n",
       " '/features/4/group3/expand3x3/conv/log_out': 0.6298540370670829,\n",
       " '/features/6/group1/squeeze/addbias/log_in': 1.2380687082846333,\n",
       " '/features/6/group1/squeeze/addbias/log_out': 1.2370265450064593,\n",
       " '/features/6/group1/squeeze/conv/log_in': 0.49993469959168924,\n",
       " '/features/6/group1/squeeze/conv/log_out': 1.2380687082846333,\n",
       " '/features/6/group2/expand1x1/addbias/log_in': 0.7597366092711921,\n",
       " '/features/6/group2/expand1x1/addbias/log_out': 0.7592978890486589,\n",
       " '/features/6/group2/expand1x1/conv/log_in': 1.2370265450064593,\n",
       " '/features/6/group2/expand1x1/conv/log_out': 0.7597366092711921,\n",
       " '/features/6/group3/expand3x3/addbias/log_in': 1.9433913343534694,\n",
       " '/features/6/group3/expand3x3/addbias/log_out': 1.9435510109728715,\n",
       " '/features/6/group3/expand3x3/conv/log_in': 1.2370265450064593,\n",
       " '/features/6/group3/expand3x3/conv/log_out': 1.9433913343534694,\n",
       " '/features/7/group1/squeeze/addbias/log_in': 1.8492567407803273,\n",
       " '/features/7/group1/squeeze/addbias/log_out': 1.8500206414170153,\n",
       " '/features/7/group1/squeeze/conv/log_in': 1.3909242885319266,\n",
       " '/features/7/group1/squeeze/conv/log_out': 1.8492567407803273,\n",
       " '/features/7/group2/expand1x1/addbias/log_in': 1.8838527409110482,\n",
       " '/features/7/group2/expand1x1/addbias/log_out': 1.8844988965612697,\n",
       " '/features/7/group2/expand1x1/conv/log_in': 1.8500206414170153,\n",
       " '/features/7/group2/expand1x1/conv/log_out': 1.8838527409110482,\n",
       " '/features/7/group3/expand3x3/addbias/log_in': 2.227365809162771,\n",
       " '/features/7/group3/expand3x3/addbias/log_out': 2.227361724132628,\n",
       " '/features/7/group3/expand3x3/conv/log_in': 1.8500206414170153,\n",
       " '/features/7/group3/expand3x3/conv/log_out': 2.227365809162771,\n",
       " '/features/9/group1/squeeze/addbias/log_in': 4.346084234282726,\n",
       " '/features/9/group1/squeeze/addbias/log_out': 4.3457396499753935,\n",
       " '/features/9/group1/squeeze/conv/log_in': 1.9297860213152067,\n",
       " '/features/9/group1/squeeze/conv/log_out': 4.346084234282726,\n",
       " '/features/9/group2/expand1x1/addbias/log_in': 2.41905777097687,\n",
       " '/features/9/group2/expand1x1/addbias/log_out': 2.418916717289001,\n",
       " '/features/9/group2/expand1x1/conv/log_in': 4.3457396499753935,\n",
       " '/features/9/group2/expand1x1/conv/log_out': 2.41905777097687,\n",
       " '/features/9/group3/expand3x3/addbias/log_in': 7.637818343996063,\n",
       " '/features/9/group3/expand3x3/addbias/log_out': 7.637851985420768,\n",
       " '/features/9/group3/expand3x3/conv/log_in': 4.3457396499753935,\n",
       " '/features/9/group3/expand3x3/conv/log_out': 7.637818343996063,\n",
       " 'input': 0.020787402400820273}"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We now have statistics for all the activations. Reduce to a scale factor for each one.\n",
    "def scale_factor_from_range(min, max, max_q_level=(2**7-1)):\n",
    "    pos_range = builtins.max(abs(min), abs(max))\n",
    "    \n",
    "    ## TRIAL lock to powers of 2\n",
    "    #pos_range = 2.0**(math.ceil(math.log2(pos_range+1e-12)))\n",
    "    \n",
    "    # val = q_val * scale_factor; so:\n",
    "    scale_factor = pos_range / max_q_level\n",
    "    return float(scale_factor)\n",
    "def scale_factor_from_stats(ranges, max_q_level=(2**7-1)):\n",
    "    min_min = np.min(ranges[0])\n",
    "    max_max = np.max(ranges[1])\n",
    "    return scale_factor_from_range(min_min, max_max, max_q_level)\n",
    "scale_factors = { k : scale_factor_from_stats(np.asarray(v.log_items))\n",
    "                  for (k,v) in loggers.items() }\n",
    "scale_factors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualise scale factors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sbs\n",
    "sbs.distplot(list(scale_factors.values()), hist=True, rug=True)\n",
    "\n",
    "#np.percentile(data, q=[0, 1, 5, 95, 99, 100], axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modifying for quantised inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions to quantise a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SqueezeNet(\n",
       "  (features): Sequential(\n",
       "    (0): Conv2d (3, 64, kernel_size=(3, 3), stride=(2, 2))\n",
       "    (1): ReLU(inplace)\n",
       "    (2): MaxPool2d(kernel_size=(3, 3), stride=(2, 2), dilation=(1, 1))\n",
       "    (3): Fire(\n",
       "      (group1): Sequential(\n",
       "        (squeeze): Conv2d (64, 16, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (squeeze_activation): ReLU(inplace)\n",
       "      )\n",
       "      (group2): Sequential(\n",
       "        (expand1x1): Conv2d (16, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (expand1x1_activation): ReLU(inplace)\n",
       "      )\n",
       "      (group3): Sequential(\n",
       "        (expand3x3): Conv2d (16, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (expand3x3_activation): ReLU(inplace)\n",
       "      )\n",
       "    )\n",
       "    (4): Fire(\n",
       "      (group1): Sequential(\n",
       "        (squeeze): Conv2d (128, 16, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (squeeze_activation): ReLU(inplace)\n",
       "      )\n",
       "      (group2): Sequential(\n",
       "        (expand1x1): Conv2d (16, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (expand1x1_activation): ReLU(inplace)\n",
       "      )\n",
       "      (group3): Sequential(\n",
       "        (expand3x3): Conv2d (16, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (expand3x3_activation): ReLU(inplace)\n",
       "      )\n",
       "    )\n",
       "    (5): MaxPool2d(kernel_size=(3, 3), stride=(2, 2), dilation=(1, 1))\n",
       "    (6): Fire(\n",
       "      (group1): Sequential(\n",
       "        (squeeze): Conv2d (128, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (squeeze_activation): ReLU(inplace)\n",
       "      )\n",
       "      (group2): Sequential(\n",
       "        (expand1x1): Conv2d (32, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (expand1x1_activation): ReLU(inplace)\n",
       "      )\n",
       "      (group3): Sequential(\n",
       "        (expand3x3): Conv2d (32, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (expand3x3_activation): ReLU(inplace)\n",
       "      )\n",
       "    )\n",
       "    (7): Fire(\n",
       "      (group1): Sequential(\n",
       "        (squeeze): Conv2d (256, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (squeeze_activation): ReLU(inplace)\n",
       "      )\n",
       "      (group2): Sequential(\n",
       "        (expand1x1): Conv2d (32, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (expand1x1_activation): ReLU(inplace)\n",
       "      )\n",
       "      (group3): Sequential(\n",
       "        (expand3x3): Conv2d (32, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (expand3x3_activation): ReLU(inplace)\n",
       "      )\n",
       "    )\n",
       "    (8): MaxPool2d(kernel_size=(3, 3), stride=(2, 2), dilation=(1, 1))\n",
       "    (9): Fire(\n",
       "      (group1): Sequential(\n",
       "        (squeeze): Conv2d (256, 48, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (squeeze_activation): ReLU(inplace)\n",
       "      )\n",
       "      (group2): Sequential(\n",
       "        (expand1x1): Conv2d (48, 192, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (expand1x1_activation): ReLU(inplace)\n",
       "      )\n",
       "      (group3): Sequential(\n",
       "        (expand3x3): Conv2d (48, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (expand3x3_activation): ReLU(inplace)\n",
       "      )\n",
       "    )\n",
       "    (10): Fire(\n",
       "      (group1): Sequential(\n",
       "        (squeeze): Conv2d (384, 48, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (squeeze_activation): ReLU(inplace)\n",
       "      )\n",
       "      (group2): Sequential(\n",
       "        (expand1x1): Conv2d (48, 192, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (expand1x1_activation): ReLU(inplace)\n",
       "      )\n",
       "      (group3): Sequential(\n",
       "        (expand3x3): Conv2d (48, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (expand3x3_activation): ReLU(inplace)\n",
       "      )\n",
       "    )\n",
       "    (11): Fire(\n",
       "      (group1): Sequential(\n",
       "        (squeeze): Conv2d (384, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (squeeze_activation): ReLU(inplace)\n",
       "      )\n",
       "      (group2): Sequential(\n",
       "        (expand1x1): Conv2d (64, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (expand1x1_activation): ReLU(inplace)\n",
       "      )\n",
       "      (group3): Sequential(\n",
       "        (expand3x3): Conv2d (64, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (expand3x3_activation): ReLU(inplace)\n",
       "      )\n",
       "    )\n",
       "    (12): Fire(\n",
       "      (group1): Sequential(\n",
       "        (squeeze): Conv2d (512, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (squeeze_activation): ReLU(inplace)\n",
       "      )\n",
       "      (group2): Sequential(\n",
       "        (expand1x1): Conv2d (64, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (expand1x1_activation): ReLU(inplace)\n",
       "      )\n",
       "      (group3): Sequential(\n",
       "        (expand3x3): Conv2d (64, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (expand3x3_activation): ReLU(inplace)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (classifier): Sequential(\n",
       "    (0): Dropout(p=0.5)\n",
       "    (1): Conv2d (512, 1000, kernel_size=(1, 1), stride=(1, 1))\n",
       "    (2): ReLU(inplace)\n",
       "    (3): AvgPool2d(kernel_size=13, stride=13, padding=0, ceil_mode=False, count_include_pad=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_orig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def quantise(x, scale_factor, saturate_at=None):\n",
    "    # Quantise the activation\n",
    "    x = x.div(scale_factor)\n",
    "    x = x.round()\n",
    "    #x = x.int()\n",
    "    # Optionally saturate into fixed bit-width\n",
    "    if saturate_at is not None:\n",
    "        x = x.clamp(-saturate_at, saturate_at)\n",
    "    \n",
    "    ## TRIAL don't compute on integers\n",
    "    x = x.mul(scale_factor)\n",
    "    \n",
    "    return x\n",
    "def quantise_const(x, scale_factor, max_q_level):\n",
    "    if scale_factor is None:\n",
    "        min = x.min().data.cpu().numpy()[0]\n",
    "        max = x.max().data.cpu().numpy()[0]\n",
    "        scale_factor = scale_factor_from_range(min, max, max_q_level)\n",
    "    x.data = quantise(x.data, scale_factor, saturate_at=max_q_level)\n",
    "    return scale_factor\n",
    "def dequantise(x, scale_factor):\n",
    "    #x = x.float()\n",
    "    x = x.mul(scale_factor)\n",
    "    return x\n",
    "\n",
    "class Quantise(nn.Module):\n",
    "    def __init__(self, scale_factor, saturate_at=None):\n",
    "        super(Quantise, self).__init__()\n",
    "        self.scale_factor = scale_factor\n",
    "        self.saturate_at = saturate_at\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = quantise(x, self.scale_factor, self.saturate_at)\n",
    "        return x\n",
    "\n",
    "class Saturate(nn.Module):\n",
    "    def __init__(self, saturate_at):\n",
    "        super(Saturate, self).__init__()\n",
    "        self.saturate_at = saturate_at\n",
    "        \n",
    "    def forward(self, x):\n",
    "        ## TRIAL don't compute on integers\n",
    "        #x = x.clamp(-self.saturate_at, self.saturate_at)\n",
    "        return x\n",
    "\n",
    "class Dequantise(nn.Module):\n",
    "    def __init__(self, scale_factor):\n",
    "        super(Dequantise, self).__init__()\n",
    "        self.scale_factor = scale_factor\n",
    "        \n",
    "    def forward(self, x):\n",
    "        ## TRIAL don't compute on integers\n",
    "        #x = dequantise(x, self.scale_factor)\n",
    "        return x\n",
    "    \n",
    "# This just locks values to quantization levels after these ops.\n",
    "# Also need to:\n",
    "#  - Quantise weights + biases\n",
    "#  - Merge Conv2d and BatchNorm weights before quantising (no BatchNorm in this SqN)\n",
    "#  - Requantize after convolutions to lower bitwidth\n",
    "#  - \n",
    "def duplicate_model_with_quantisation(model):\n",
    "    def insert_quantise(m, path):\n",
    "        # (nn.Conv2d, nn.Linear, nn.BatchNorm1d, nn.BatchNorm2d, nn.AvgPool2d)\n",
    "        if isinstance(m, (nn.Conv2d)):\n",
    "            sf_conv_in  = scale_factors[f'{path}/conv/log_in']\n",
    "            sf_conv_out  = scale_factors[f'{path}/conv/log_out']\n",
    "            sf_bias_in = scale_factors[f'{path}/addbias/log_in']\n",
    "            \n",
    "            # Quantise Conv2d weights to 8 bits\n",
    "            bw_conv_in = 8\n",
    "            sf_weight = quantise_const(m.weight, scale_factor=None, max_q_level=2**(bw_conv_in-1)-1)\n",
    "            \n",
    "            # Bit-width after conv\n",
    "            bw_conv_out = (2*bw_conv_in + \n",
    "                           int(math.ceil(math.log2(m.kernel_size[0] *\n",
    "                                                   m.kernel_size[1] * \n",
    "                                                   m.in_channels))))\n",
    "            \n",
    "            # Quantise Conv2d biases to any number of bits, but the same sf as the activation\n",
    "            sf_bias = sf_conv_in * sf_weight\n",
    "            #if (sf_bias != sf_conv_out):\n",
    "            #    print(f\"WARNING: calculated scaling factor doesn't match that observed:\")\n",
    "            #    print(f\"         {sf_bias} != {sf_conv_out}\")\n",
    "            \n",
    "            if m.bias is not None:\n",
    "                ## TRIAL don't compute on integers\n",
    "                #quantise_const(m.bias, sf_bias, max_q_level=2**32-1)\n",
    "                quantise_const(m.bias, None, max_q_level=2**7-1)\n",
    "            \n",
    "            return nn.Sequential(# Quantise input to 8 bits\n",
    "                                 Quantise(sf_conv_in, saturate_at=2**(bw_conv_in-1)-1),\n",
    "                                 # Perform conv + bias\n",
    "                                 m,\n",
    "                                 # Saturate to emulate fixed bit-width\n",
    "                                 Saturate(2**(bw_conv_out-1)-1),\n",
    "                                 # Dequantise\n",
    "                                 Dequantise(sf_bias))\n",
    "        elif isinstance(m, (AddBias)):\n",
    "            print(\"ERROR: please use split convs for logging, and original model for quantizing\")\n",
    "            exit(1)\n",
    "    sf_input = scale_factors['input']\n",
    "    return nn.Sequential(# Quantise the input, then dequantise\n",
    "                         Quantise(sf_input, saturate_at=2**7-1),\n",
    "                         Dequantise(sf_input),\n",
    "                         # The rest of the model (with quantisation)\n",
    "                         model_apply(model, insert_quantise))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quantise the model and evaluate the drop in accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quantise model\n",
    "model_quant = duplicate_model_with_quantisation(model_orig)\n",
    "# Evaluate accuracy\n",
    "val_ds = ds_fetcher(batch_size, data_root=data_root, train=False, input_size=input_size)\n",
    "acc1q, acc5q = misc.eval_model(model_quant, val_ds, ngpu=ngpu, is_imagenet=is_imagenet, n_sample=100)\n",
    "print(f'Test accuracy of ({acc1q},{acc5q}), a drop of ({acc1-acc1q},{acc5-acc5q})')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment with the pow2-scale-factor linear quantisation scheme from the playground"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_quantize(input, sf, bits):\n",
    "    assert bits >= 1, bits\n",
    "    if bits == 1:\n",
    "        return torch.sign(input) - 1\n",
    "    delta = math.pow(2.0, -sf)\n",
    "    bound = math.pow(2.0, bits-1)\n",
    "    min_val = - bound\n",
    "    max_val = bound - 1\n",
    "    \n",
    "    x = input.div(delta)\n",
    "    x.round_()\n",
    "    if isinstance(x, Variable):\n",
    "        x.data.clamp_(min_val, max_val)\n",
    "    else:\n",
    "        x.clamp_(min_val, max_val)\n",
    "    x.mul_(delta)\n",
    "    return x\n",
    "\n",
    "class LinearQuant(nn.Module):\n",
    "    def __init__(self, sf, bits):\n",
    "        super(LinearQuant, self).__init__()\n",
    "        self.bits = bits\n",
    "        self.sf = sf\n",
    "\n",
    "    def forward(self, input):\n",
    "        output = linear_quantize(input, self.sf, self.bits)\n",
    "        return output\n",
    "\n",
    "    def __repr__(self):\n",
    "        return '{}(sf={}, bits={})'.format(\n",
    "            self.__class__.__name__, self.sf, self.bits)\n",
    "\n",
    "def duplicate_model_with_q(model):\n",
    "    def convert_sf(sf, pow2=True):\n",
    "        l = math.log2( sf * (2**7-1) / (2**7) )\n",
    "        if pow2:\n",
    "            return -math.ceil(l)\n",
    "        else:\n",
    "            return -l\n",
    "    def insert_quantise(m, path):\n",
    "        pow2bias       = False\n",
    "        pow2weight     = False\n",
    "        pow2activation = True\n",
    "        bw_bias        = 32\n",
    "        \n",
    "        # (nn.Conv2d, nn.Linear, nn.BatchNorm1d, nn.BatchNorm2d, nn.AvgPool2d)\n",
    "        if isinstance(m, (nn.Conv2d)):\n",
    "            sf_conv_in  = scale_factors[f'{path}/conv/log_in']\n",
    "            sf_conv_out  = scale_factors[f'{path}/conv/log_out']\n",
    "            sf_bias_in = scale_factors[f'{path}/addbias/log_in']\n",
    "            \n",
    "            # Quantise Conv2d weights to 8 bits\n",
    "            bw_conv_in = 8\n",
    "#             sf_weight = quantise_const(m.weight, scale_factor=None, max_q_level=2**(bw_conv_in-1)-1)\n",
    "            ## TRIAL copy ptpg\n",
    "            #sf_weight = bw_conv_in - 1. - quant.compute_integral_part(m.weight, overflow_rate=0)\n",
    "            sf_weight = quantise_const(deepcopy(m.weight), scale_factor=None, max_q_level=2**(bw_conv_in-1))\n",
    "            sf_weight = convert_sf(sf_weight, pow2=pow2weight)\n",
    "            m.weight.data = linear_quantize(m.weight.data, sf_weight, bits=bw_conv_in)\n",
    "            \n",
    "            # Bit-width after conv\n",
    "            bw_conv_out = (2*bw_conv_in + \n",
    "                           int(math.ceil(math.log2(m.kernel_size[0] *\n",
    "                                                   m.kernel_size[1] * \n",
    "                                                   m.in_channels))))\n",
    "            \n",
    "            # Quantise Conv2d biases to any number of bits, but the same sf as the activation\n",
    "#             sf_bias = sf_conv_in * sf_weight\n",
    "#             if m.bias is not None:\n",
    "#                 ## TRIAL don't compute on integers\n",
    "#                 #quantise_const(m.bias, sf_bias, max_q_level=2**32-1)\n",
    "#                 quantise_const(m.bias, None, max_q_level=2**7-1)\n",
    "            ## TRIAL copy ptpg\n",
    "            #sf_bias = bw_bias - 1. - quant.compute_integral_part(m.bias, overflow_rate=0)\n",
    "            sf_bias = quantise_const(deepcopy(m.bias), scale_factor=None, max_q_level=2**(bw_bias-1))\n",
    "            sf_bias = convert_sf(sf_bias, pow2=pow2bias)\n",
    "            m.bias.data = linear_quantize(m.bias.data, sf_bias, bits=bw_bias)\n",
    "            \n",
    "#             return nn.Sequential(# Quantise input to 8 bits\n",
    "#                                  Quantise(sf_conv_in, saturate_at=2**(bw_conv_in-1)-1),\n",
    "#                                  # Perform conv + bias\n",
    "#                                  m,\n",
    "#                                  # Saturate to emulate fixed bit-width\n",
    "#                                  Saturate(2**(bw_conv_out-1)-1),\n",
    "#                                  # Dequantise\n",
    "#                                  Dequantise(sf_bias))\n",
    "            #sf_2 = convert_sf(scale_factors[f'{path}/addbias/log_out'])\n",
    "            sf_2 = convert_sf(scale_factors[f'{path}/conv/log_out'], pow2=pow2activation)\n",
    "            return nn.Sequential(m,\n",
    "                                 LinearQuant(sf=sf_2, bits=8))\n",
    "        elif isinstance(m, (AddBias)):\n",
    "            print(\"ERROR: please use split convs for logging, and original model for quantizing\")\n",
    "            exit(1)\n",
    "        elif isinstance(m, (nn.AvgPool2d)):\n",
    "            sf_2 = convert_sf(scale_factors[f'{path}/log_out'], pow2=pow2activation)\n",
    "            return nn.Sequential(m,\n",
    "                                 LinearQuant(sf=sf_2, bits=8))\n",
    "            \n",
    "#     sf_input = scale_factors['input']\n",
    "#     return nn.Sequential(# Quantise the input, then dequantise\n",
    "#                          Quantise(sf_input, saturate_at=2**7-1),\n",
    "#                          Dequantise(sf_input),\n",
    "#                          # The rest of the model (with quantisation)\n",
    "#                          model_apply(model, insert_quantise))\n",
    "    sf_input = convert_sf(scale_factors['input'])\n",
    "    return nn.Sequential(#LinearQuant(sf=sf_input, bits=8),\n",
    "                         model_apply(model, insert_quantise))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading pickle object from /tmp/public_dataset/pytorch/imagenet-data/val224.pkl\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building IMAGENET data loader: 50,000 for train; 50,000 for test\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "=> Done (5.9057 s)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "325de5826f2b437096c4898e6f7107fd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/html": [
       "<p>Failed to display Jupyter Widget of type <code>HBox</code>.</p>\n",
       "<p>\n",
       "  If you're reading this message in the Jupyter Notebook or JupyterLab Notebook, it may mean\n",
       "  that the widgets JavaScript is still loading. If this message persists, it\n",
       "  likely means that the widgets JavaScript library is either not installed or\n",
       "  not enabled. See the <a href=\"https://ipywidgets.readthedocs.io/en/stable/user_install.html\">Jupyter\n",
       "  Widgets Documentation</a> for setup instructions.\n",
       "</p>\n",
       "<p>\n",
       "  If you're reading this message in another frontend (for example, a static\n",
       "  rendering on GitHub or <a href=\"https://nbviewer.jupyter.org/\">NBViewer</a>),\n",
       "  it may mean that your frontend doesn't currently support widgets.\n",
       "</p>\n"
      ],
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=500), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy of (0.54484,0.77956), a drop of (0.014639999999999986,0.011739999999999973)\n"
     ]
    }
   ],
   "source": [
    "# Quantise model\n",
    "model_quant = duplicate_model_with_q(model_orig)\n",
    "# Evaluate accuracy\n",
    "val_ds = ds_fetcher(batch_size, data_root=data_root, train=False, input_size=input_size)\n",
    "acc1q2, acc5q2 = misc.eval_model(model_quant, val_ds, ngpu=ngpu, is_imagenet=is_imagenet)#, n_sample=100)\n",
    "print(f'Test accuracy of ({acc1q2},{acc5q2}), a drop of ({acc1-acc1q2},{acc5-acc5q2})')\n",
    "#   bias    | non-pow2 sf |    Test Accuracy   |      Drop\n",
    "# bit-width |  b  w  a    |   top-1    top-5   |   top-1    top-5\n",
    "#===========+=============+====================+==================\n",
    "#    8      |  -  -  -    |  0.53834  0.77446  |  0.02114  0.01684\n",
    "#   32      |  -  -  -    |  0.53882  0.77456  |  0.02066  0.01674\n",
    "#    8      |  Y  -  -    |  0.53756  0.77416  |  0.02192  0.01714\n",
    "#   32      |  Y  -  -    |  0.53906  0.77370  |  0.02042  0.01760\n",
    "#    8      |  -  Y  -    |  0.54574  0.77916  |  0.01374  0.01214\n",
    "#    8      |  Y  Y  -    |  0.54616  0.77828  |  0.01332  0.01302\n",
    "#   32      |  Y  Y  -    |  0.54484  0.77956  |  0.01464  0.01174\n",
    "#    8      |  -  -  Y    |  0.54594  0.78008  |  0.01354  0.01122\n",
    "#    8      |  Y  Y  Y    |  0.55156  0.78526  |  0.00792  0.00604\n",
    "#   32      |  Y  Y  Y    |  0.55294  0.78618  |  0.00654  0.00512"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------\n",
    "# Random code & notes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_ds = ds_fetcher(batch_size, data_root=data_root, train=False, input_size=input_size)\n",
    "(data, target) = next(val_ds)\n",
    "data = Variable(torch.FloatTensor(data)).cuda()\n",
    "model_cuda = torch.nn.DataParallel(model_raw.eval(), device_ids=range(ngpu)).cuda()\n",
    "trace, out = jit.trace(model_cuda, data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trace.graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(trace)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We do dumb quantisation; no fine-tuning.\n",
    "\n",
    "weights_bw    = 8\n",
    "biases_bw     = 32\n",
    "activation_bw = 8\n",
    "overflow_rate = 0.0\n",
    "n_sample      = 20\n",
    "quant_method  = \"linear\"\n",
    "    \n",
    "def duplicate_model_with_quant(model, bits, overflow_rate=0.0, counter=10, type='linear'):\n",
    "    \"\"\"assume that original model has at least a nn.Sequential\"\"\"\n",
    "    assert type in ['linear', 'minmax', 'log', 'tanh']\n",
    "    if isinstance(model, nn.Sequential):\n",
    "        print(f'> Sequential')\n",
    "        l = OrderedDict()\n",
    "        for k, v in model._modules.items():\n",
    "            print(f'Looking at : {k}')\n",
    "            if isinstance(v, (nn.Conv2d, nn.Linear, nn.BatchNorm1d, nn.BatchNorm2d, nn.AvgPool2d)):\n",
    "                parameters = list(v.parameters())\n",
    "                if isinstance(v, nn.Conv2d):\n",
    "                    print(f\"Found Conv2D:\")\n",
    "                    print(f\"  weights: [o,i,k,k] {parameters[0].shape}\")\n",
    "                    print(f\"  biases : [o] {parameters[1].shape}\")\n",
    "                l[k] = v\n",
    "                \n",
    "            else:\n",
    "                l[k] = duplicate_model_with_quant(v, bits, overflow_rate, counter, type)\n",
    "        m = nn.Sequential(l)\n",
    "        return m\n",
    "    else:\n",
    "        print(f'> Not Sequential')\n",
    "        for k, v in model._modules.items():\n",
    "            model._modules[k] = duplicate_model_with_quant(v, bits, overflow_rate, counter, type)\n",
    "        return model\n",
    "\n",
    "model_quant = duplicate_model_with_quant(model_raw, bits=activation_bw, overflow_rate=overflow_rate,\n",
    "                                               counter=n_sample, type=quant_method)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# eval model\n",
    "val_ds = ds_fetcher(batch_size, data_root=data_root, train=False, input_size=input_size)\n",
    "acc1q, acc5q = misc.eval_model(model_quant, val_ds, ngpu=ngpu, is_imagenet=is_imagenet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc1q, acc5q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exporting model\n",
    "There is the existing ONNX exporter. Either modify that, or manually add functions to each type of Module and call them recursively."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MobileNet\n",
    "Search GitHub; there seem to be plenty of implementations. E.g. https://github.com/marvis/pytorch-mobilenet\n",
    "There are also implementations for v2.\n",
    "\n",
    "## MobileNet-SSD\n",
    "Try modifying https://github.com/amdegroot/ssd.pytorch:\n",
    "  - Use MobileNet feature extractor\n",
    "  - Use dw convolutions in SSD (SSDLite)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_raw.state_dict().keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (conda_pytorch_p36)",
   "language": "python",
   "name": "conda_pytorch_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
